---
description: Six diagnostic patterns map operational symptoms to structural causes and prescribed responses, converting accumulated observations into a decision protocol rather than an undifferentiated pile
kind: research
topics: ["[[design-dimensions]]", "[[maintenance-patterns]]"]
methodology: ["Original"]
source: [[knowledge-system-derivation-blueprint]]
---

# evolution observations provide actionable signals for system adaptation

A derived knowledge system generates operational data from the moment it starts running. Notes accumulate, fields get filled or skipped, agents navigate or fail to navigate, processing produces output that either integrates or sits orphaned. The question is whether this operational data remains an undifferentiated pile of experience or whether it becomes diagnostic intelligence that tells you specifically what is wrong and what to do about it.

The diagnostic protocol maps six observation patterns to their structural causes and prescribed responses:

| Observation | What It Signals | Action |
|-------------|----------------|--------|
| Note type unused for 30+ days | Over-modeled | Consider removing or merging |
| Field consistently filled with "N/A" | Required field not useful | Demote to optional |
| Field manually added to 20%+ of notes | Organic schema growth | Add to template |
| Agent cannot find note within 3 nav steps | Navigation failure | Review MOC structure |
| Processing produces notes that sit unlinked | Processing doesn't match domain value pattern | Rethink processing phase |
| MOC exceeds threshold (50 agent / 35 human) | Navigation overload | Split into sub-MOCs |

What makes this more than a troubleshooting checklist is that each row connects a surface symptom to a structural cause. An unused note type is not a content problem but a modeling problem -- the derivation hypothesized a distinction the domain does not actually need. Fields filled with "N/A" are not lazy operators but schema overreach -- the required/optional boundary was drawn in the wrong place. Unlinked processing output is not a connection-finding failure but a processing design mismatch -- since [[every knowledge domain shares a four-phase processing skeleton that diverges only in the process step]], the pipeline produces artifacts that the domain's value pattern does not naturally integrate because the process step was designed for a different kind of transformation than the domain actually needs. This is the runtime signature of what [[false universalism applies same processing logic regardless of domain]] identifies as the most insidious derivation failure: the operations are technically executable but semantically empty for the target domain.

This diagnostic structure transforms the relationship between observation and action. Since [[hook-driven learning loops create self-improving methodology through observation accumulation]], the vault already has a mechanism for accumulating raw observations -- hooks nudge capture, observations pile up, rethink reviews the pile. But accumulation without interpretation produces a growing evidence base that requires increasingly expensive pattern recognition to extract actionable signals. The diagnostic protocol provides the interpretation layer: instead of asking "what patterns emerge from these 40 observations?", you can ask "does any observation match a known diagnostic?" The first question requires synthesis. The second requires lookup. Both are necessary, but the lookup path handles the common cases efficiently so that synthesis capacity is reserved for genuinely novel patterns.

There is a category the six diagnostics do not explicitly cover: automation infrastructure failures. Since [[observation and tension logs function as dead-letter queues for failed automation]], when qmd crashes, rename scripts miss links, or schema migrations skip notes, the failure gets captured as an observation or tension note — but these entries require different triage than the evolution signals the diagnostic table addresses. A qmd crash demands immediate repair or workaround documentation; an unused note type informs a design change on a quarterly cadence. The dead-letter framing suggests the diagnostic protocol needs a seventh row: infrastructure failure entries that require urgency-based triage rather than the scheduled cadence appropriate for evolution signals.

The protocol also reveals something about how derived systems should evolve. Each diagnostic points in a specific direction: toward simplification (remove unused types, demote useless fields), toward organic growth (promote emergent fields), or toward restructuring (reorganize navigation, redesign processing). These are not random maintenance tasks but evolution pressures that push the system toward fit with its actual use rather than its designed-for use. The diagnostics also serve as the interpretation layer for [[friction-driven module adoption prevents configuration debt by adding complexity only at pain points]] — the five-repetition threshold that determines when a module should be added depends on structured observation rather than intuitive friction sensing. Without the diagnostic protocol, counting repetitions degenerates into counting activities without understanding what they mean, and the friction signal loses the specificity that makes it actionable. Since [[derived systems follow a seed-evolve-reseed lifecycle]], the diagnostics serve as the feedback mechanism for the evolution phase: they tell the agent whether adaptations are incremental corrections (still in evolution) or whether accumulated drift has produced systemic incoherence (reseeding is needed). The derivation process produces a hypothesis about what the domain needs. Evolution observations test that hypothesis against operational reality. The diagnostics close the feedback loop by converting test results into specific structural modifications.

Since [[community detection algorithms can inform when MOCs should split or merge]], one row of the diagnostic table -- MOC threshold exceeded -- already has a more sophisticated treatment in the vault. Community detection provides algorithmic signals for reorganization that go beyond simple note counts. This suggests the diagnostic protocol is a starting layer, not a final word. Each row can be deepened: the navigation failure diagnostic could incorporate graph traversal metrics, the schema diagnostics could use field completion rates, the processing mismatch diagnostic could track link density of processed output over time. The schema diagnostics in particular have a more developed treatment: since [[schema evolution follows observe-then-formalize not design-then-enforce]], the quarterly review protocol specifies five concrete signals -- manual field additions, placeholder stuffing, unused enums, patterned free text, oversized MOCs -- that refine three of the six diagnostics here with specific thresholds and evidence-gathering cadence. The protocol's value is not exhaustiveness but the structural insight that operational symptoms have specific structural causes.

The distinction between note-level and system-level maintenance matters here. Since [[schema enforcement via validation agents enables soft consistency]], note-level quality is monitored through schema validation -- individual notes checked against their templates. Since [[backward maintenance asks what would be different if written today]], note-level evolution is handled through reweaving -- individual notes reconsidered against current understanding. Evolution diagnostics operate at a different level entirely: they monitor whether the system's structural decisions -- which note types exist, which fields are required, how MOCs are organized, how processing phases work -- still match operational reality. A note can pass every schema check and still be evidence of a system-level problem if it belongs to an over-modeled type that nobody uses.

The diagnostic rows also serve as the desired-state declarations for a reconciliation architecture. Since [[reconciliation loops that compare desired state to actual state enable drift correction without continuous monitoring]], each row in the table above specifies what healthy looks like (no unused types, no N/A-stuffed fields, all notes reachable within three navigation steps), how to detect divergence (count-based checks, field completion rates, navigation path analysis), and what remediation to apply. The reconciliation loop is the scheduling infrastructure that runs these diagnostics systematically on a cadence rather than waiting for someone to notice symptoms. But not all diagnostics need the same cadence. Since [[maintenance scheduling frequency should match consequence speed not detection capability]], the appropriate frequency for each diagnostic depends on how fast the underlying problem propagates. Unused note types develop over months as domain understanding evolves — monthly detection suffices. N/A field rates accumulate over multiple sessions as templates encounter real content — weekly checks catch the drift before it compounds. Navigation failure, however, can develop within a single session as new notes shift the graph's navigability — session-start dashboard checks are the appropriate tier. The deterministic diagnostics (unused type count, N/A field rate, manual addition percentage) are candidates for automated scheduled detection at their consequence-matched frequencies, while the judgment-requiring diagnostics (navigation failure, processing mismatch) need agent-level evaluation at their (often more frequent) scheduled intervals.

The deeper implication is that since [[derivation generates knowledge systems from composable research claims not template customization]], derivation is not a one-time event but a hypothesis that needs ongoing testing. The initial derivation maps domain needs to configuration choices. Evolution observations measure whether those choices were correct. The diagnostics convert measurements into corrections. And the corrected system generates new observations that test the corrections. This is the same self-improving loop that hook-driven learning enables at the methodology level, but applied to the system architecture itself -- the structure of the knowledge system becomes subject to evidence-based revision rather than remaining fixed once derived.

There is genuine uncertainty about whether these six diagnostics are sufficient or whether they cover the most important failure modes. Navigation failure and processing mismatch seem like the most consequential signals because they indicate fundamental design errors, while schema diagnostics (unused types, N/A fields, emergent fields) are more incremental adjustments. The protocol would benefit from weighting -- not all observations are equally urgent. A field filled with "N/A" can wait; agents unable to find notes within three navigation steps suggests the system is failing at its primary purpose and demands immediate attention. This urgency distinction connects to a broader risk: since [[metacognitive confidence can diverge from retrieval capability]], structural metrics like link density and schema compliance can show a healthy system while navigation failure and processing mismatch go undetected because they require actually attempting the operations the system is supposed to support. The diagnostic protocol is an anti-divergence mechanism that tests functional capability rather than structural appearance.

The diagnostics themselves are candidates for the encoding trajectory. Since [[methodology development should follow the trajectory from documentation to skill to hook as understanding hardens]], the deterministic diagnostics -- unused note types (count-based), N/A field rates (frequency-based), manual field additions (percentage-based) -- could eventually be encoded as hooks that fire automatically and surface warnings. But the judgment-requiring diagnostics -- navigation failure assessment and processing mismatch evaluation -- should remain at the skill level because they require contextual evaluation that varies with the system's current state and the agent's current understanding of domain value patterns. And since [[confidence thresholds gate automated action between the mechanical and judgment zones]], the encoding trajectory gains a middle tier: diagnostics that are not fully deterministic but can score their own certainty could operate in the confidence-gated zone, auto-remediating when confidence is high (a note type truly unused for 90 days in an active domain) while only suggesting when confidence is medium (a note type unused for 30 days in a domain with seasonal patterns). Beyond confidence scoring, since [[the fix-versus-report decision depends on determinism reversibility and accumulated trust]], each diagnostic remediation must also pass the four conjunctive conditions before auto-applying: removing an unused note type is reversible via git and low-cost if wrong, but demoting a required field has higher stakes because downstream processing may depend on it — the cost condition acts as an independent veto even when confidence is high.
---

Relevant Notes:
- [[hook-driven learning loops create self-improving methodology through observation accumulation]] -- provides the accumulation mechanism; this note provides the interpretation framework that converts accumulated observations into targeted system changes
- [[community detection algorithms can inform when MOCs should split or merge]] -- one specific diagnostic (MOC threshold) generalized through algorithmic monitoring; this note frames the broader pattern where multiple diagnostics compose into a protocol
- [[backward maintenance asks what would be different if written today]] -- the reconsideration mental model operates per-note; this diagnostic protocol operates per-system, identifying which structural components need the reconsideration pass
- [[schema enforcement via validation agents enables soft consistency]] -- sibling pattern: validation agents check note-level schema compliance, evolution diagnostics check system-level structural fitness; both are asynchronous maintenance that surfaces issues without blocking
- [[derived systems follow a seed-evolve-reseed lifecycle]] -- the diagnostics are the feedback mechanism for the evolution phase: they tell you whether accumulated adaptations have drifted the configuration into incoherence requiring reseeding
- [[schema evolution follows observe-then-formalize not design-then-enforce]] -- domain-specific refinement: the schema signal table (manual additions, placeholder stuffing, unused enums) deepens three of the six diagnostics with concrete quarterly review protocol
- [[metacognitive confidence can diverge from retrieval capability]] -- the navigation failure and processing mismatch diagnostics are anti-divergence mechanisms that catch system-level failures structural metrics would miss
- [[methodology development should follow the trajectory from documentation to skill to hook as understanding hardens]] -- the diagnostics themselves could follow this trajectory: deterministic checks (unused type count, N/A field rate) could become hooks while judgment-requiring diagnostics (processing mismatch) should remain skill-level
- [[derivation generates knowledge systems from composable research claims not template customization]] -- derivation produces the configuration hypothesis that these diagnostics test: each diagnostic row measures whether a derivation choice was correct
- [[every knowledge domain shares a four-phase processing skeleton that diverges only in the process step]] -- the processing mismatch diagnostic targets the process step specifically: unlinked output signals that the process step was designed for a different transformation than the domain needs
- [[false universalism applies same processing logic regardless of domain]] -- names the derivation anti-pattern that the processing mismatch diagnostic detects: unlinked output is the runtime signal that the process step was designed for a different domain's transformation, making false universalism diagnosable even when the derivation reasoning looked sound
- [[module deactivation must account for structural artifacts that survive the toggle]] -- identifies a specific cause of two diagnostic signals: ghost fields from deactivated modules trigger the unused-type and N/A-field rows, making deactivation artifacts a named structural cause alongside over-modeling and schema overreach
- [[friction-driven module adoption prevents configuration debt by adding complexity only at pain points]] — the adoption pattern these diagnostics serve: the five-repetition threshold depends on structured friction detection to distinguish genuine pain from noise, making the diagnostic protocol the interpretation layer that operationalizes friction-driven adoption
- [[reconciliation loops that compare desired state to actual state enable drift correction without continuous monitoring]] — scheduling infrastructure: each diagnostic row is a desired-state declaration that reconciliation operationalizes by running checks systematically on a cadence rather than waiting for symptoms
- [[confidence thresholds gate automated action between the mechanical and judgment zones]] -- response graduation: the diagnostics themselves range from deterministic (unused type count, N/A field rate) to judgment-requiring (navigation failure, processing mismatch), and confidence thresholds determine which diagnostics can trigger automated remediation versus which should only suggest or log, applying the three-tier response pattern to the diagnostic protocol itself
- [[maintenance scheduling frequency should match consequence speed not detection capability]] — cadence calibration: the six diagnostics have different consequence speeds and therefore different optimal detection frequencies — unused types develop over months (slow consequence, monthly check), N/A field rates drift over weeks (multi-session, weekly), while navigation failure can develop per-session as new notes arrive (session-scale, per-session dashboard)
- [[observation and tension logs function as dead-letter queues for failed automation]] — gap identification: the dead-letter framing reveals that the diagnostic protocol covers evolution signals but not automation infrastructure failures; qmd crashes and script failures require urgency-based triage rather than the scheduled cadence appropriate for design evolution, suggesting a seventh diagnostic category
- [[the fix-versus-report decision depends on determinism reversibility and accumulated trust]] — remediation gating for diagnostics: the four conjunctive conditions determine which diagnostic actions can auto-apply (deterministic, reversible, low-cost, trusted) versus which should only report; complements confidence thresholds by adding independent vetoes for cost and reversibility that confidence scoring alone cannot capture

Topics:
- [[design-dimensions]]
- [[maintenance-patterns]]
