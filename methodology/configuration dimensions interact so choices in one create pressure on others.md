---
description: Atomic granularity forces explicit linking, deep navigation, and heavy processing — the valid space is far smaller than the combinatorial product because each choice constrains its neighbors
kind: research
topics: ["[[design-dimensions]]"]
methodology: ["Original"]
source: [[knowledge-system-derivation-blueprint]]
---

# configuration dimensions interact so choices in one create pressure on others

Knowledge system design involves at least eight dimensions — since [[eight configuration dimensions parameterize the space of possible knowledge systems]], the list includes granularity, organization, linking philosophy, processing intensity, navigation depth, maintenance cadence, schema density, and automation level. The tempting assumption is that these are independent knobs: pick your preferred setting on each, combine them, and deploy. But the dimensions are coupled. A choice at one end of one spectrum creates pressure toward specific regions of other spectra, and ignoring that pressure produces systems that are internally incoherent.

The clearest example is granularity cascading through everything else. Choosing atomic notes (one claim per file) means each note has minimal internal context, so connections between notes must be explicit — you cannot rely on "it's in the same document" proximity. This forces explicit linking. But thousands of atomic notes with dense explicit links require deep navigation structures to remain traversable, because since [[navigational vertigo emerges in pure association systems without local hierarchy]], a flat sea of equally-small nodes becomes disorienting without MOC hierarchy to provide landmarks. And maintaining all those links and navigation structures demands heavy processing — extraction pipelines, reflection passes, reweaving cycles. The granularity choice didn't just set one parameter. It created pressure across linking, navigation, and processing simultaneously. And since [[every knowledge domain shares a four-phase processing skeleton that diverges only in the process step]], the processing intensity cascade is particularly consequential — the process step is the only phase that varies by domain, so dimension interactions concentrate at that single variable point while capture, connect, and verify absorb pressure as structural constants.

The reverse cascade is equally constraining. Coarse granularity (one note per source, or topic-level documents) means each note carries enough internal context that lightweight linking suffices — a few cross-references rather than dense typed connections. Navigation can be shallow because fewer nodes need organizing. Processing can be lighter because each note is more self-contained. The entire configuration coheres at the coarse pole just as it coheres at the atomic pole, but mixing atomic granularity with shallow navigation produces an incoherent system where notes are too small to be self-contained and too numerous to find.

Automation level creates a parallel cascade. Full automation (hooks, skills, pipelines) enables dense schemas because validation catches errors that humans would miss. It enables heavy processing because pipelines handle volume that manual invocation cannot sustain. But manual operation — where every action requires explicit invocation — pressures toward minimal schemas (less to remember, less to validate by hand) and lighter processing (each step is expensive in attention). Deploying dense schemas in a manual system creates a maintenance burden that collapses under its own weight. Deploying minimal schemas in a fully automated system wastes the infrastructure's enforcement capacity. The cascade operates even within automation — since [[skill context budgets constrain knowledge system complexity on agent platforms]], limited skill slots force methodology back into instruction encoding, which degrades enforcement strength, which pressures toward simpler schemas. Since [[methodology development should follow the trajectory from documentation to skill to hook as understanding hardens]], the automation dimension itself has internal structure where different encoding levels create different pressure profiles.

Volume introduces a third pressure axis. Since [[small-world topology requires hubs and dense local links]], a 500-note vault needs deep navigation, semantic search, and automated maintenance to remain traversable. A 50-note vault works with shallow navigation and grep because the agent can hold the entire structure in context. The volume dimension doesn't just affect search modality — it pressures navigation depth and maintenance cadence simultaneously, because what works at small scale (manual, shallow, grep-based) becomes unnavigable at large scale.

The implication for derivation is that the valid configuration space is much smaller than the combinatorial product. Eight dimensions with even three positions each produces 6,561 theoretical combinations. But most are incoherent — atomic granularity with shallow navigation, manual operation with dense schemas, high volume with no automated maintenance. Since [[derivation generates knowledge systems from composable research claims not template customization]], a derivation engine that treats dimensions as independent will produce specifications that look reasonable in isolation but fail in practice because the pressures conflict. But the coupling structure cuts both ways: since [[configuration paralysis emerges when derivation surfaces too many decisions]], the same interaction constraints that make independent dimension selection incoherent also make inference tractable — resolving a few primary choices propagates through the coupling to determine secondary ones, so the derivation engine can reduce the decision surface from the full combinatorial product to the genuine choice points where user constraints leave multiple viable paths. Since [[knowledge system architecture is parameterized by platform capabilities not fixed by methodology]], the generator must understand not just which parameters to set but which parameter combinations form coherent operating points.

This reframes methodology traditions as discovered coherence points rather than arbitrary preferences — a framing that [[methodology traditions are named points in a shared configuration space not competing paradigms]] develops extensively. Zettelkasten coheres at the atomic-explicit-deep-heavy end. PARA coheres at the coarse-lightweight-shallow-manual end. Each tradition represents a region of the configuration space where the dimension interactions have been resolved through practice. The value of understanding interactions is that it enables generating NEW coherent configurations for novel use cases — combinations that no existing methodology has discovered but that respect the coupling constraints.

An upstream constraint narrows the space further: since [[storage versus thinking distinction determines which tool patterns apply]], the storage/thinking classification determines which coherence regions are even viable before dimension tuning begins. A storage system cannot coherently adopt atomic granularity with heavy processing because its purpose does not generate the synthesis demand that justifies the processing cost. A thinking system cannot coherently adopt coarse granularity with minimal linking because it sacrifices the composability that synthesis requires. The classification constrains which cascade patterns produce functional systems. And since [[complex systems evolve from simple working systems]], even within a coherent region, Gall's Law adds a temporal constraint: start at the simple end and let friction drive elaboration rather than deploying the full coherent configuration from day one.

Multi-domain composition introduces a further pressure axis that operates across domain boundaries. Since [[multi-domain systems compose through separate templates and shared graph]], when multiple domains coexist in one graph, dimension settings in one domain can conflict with settings in another. A domain with high processing intensity and dense schemas shares graph space with a domain using light processing and minimal schemas, and the shared navigation structure must serve both. The cross-domain surface area grows quadratically with each new domain, making dimension interaction analysis not just intra-system but inter-domain. Whether the shared graph mechanics — wiki links, MOCs, progressive disclosure — can bridge domains with different structural densities is an open question that tests how hard the coupling constraints really are.

Dimension coupling also explains why evolution eventually requires reseeding. Since [[derived systems follow a seed-evolve-reseed lifecycle]], small adaptations in one dimension accumulate pressure on others — adding a schema field creates query expectations, adding a MOC shifts navigation patterns. Each adaptation is locally justified but the accumulated pressure drifts the configuration into an incoherent region. This is dimension interaction operating over time: the coupling constraints that make initial derivation a constraint-satisfaction problem also make evolution a drift-detection problem, because each incremental change shifts the system's position in configuration space without checking whether the new position still satisfies the coupling constraints.

There is a shadow side. If dimension interactions are strong enough, they may reduce the space of viable configurations to essentially the known methodology traditions, making "novel derivation" more theoretical than practical. The test is whether the interaction constraints are hard (violating them produces failure) or soft (violating them produces friction that can be overcome with compensating mechanisms). Since [[four abstraction layers separate platform-agnostic from platform-dependent knowledge system features]], automation can compensate for some mismatches — automated linking can sustain atomic granularity with less navigation depth than the pure interaction would demand. The real question is how much automation can compensate versus how much the coupling is fundamental. The novel domain case tests this directly — since [[novel domains derive by mapping knowledge type to closest reference domain then adapting]], adapting a reference domain means adjusting specific dimensions, and each adjustment must respect the coupling constraints or produce an incoherent system. Whether adaptation can safely deviate from the reference coherence point measures exactly how hard the interaction constraints are.

---
---

Relevant Notes:
- [[knowledge system architecture is parameterized by platform capabilities not fixed by methodology]] — establishes that dimensions are parameterized, this note adds that parameters are coupled rather than independent
- [[eight configuration dimensions parameterize the space of possible knowledge systems]] — defines the dimensions this note shows are coupled; that note treats them as largely independent in definition, this note develops their practical entanglement
- [[methodology traditions are named points in a shared configuration space not competing paradigms]] — interaction pressure explains WHY traditions cohere where they do: each tradition resolved the coupling through practice, which this note formalizes as constraint satisfaction
- [[derivation generates knowledge systems from composable research claims not template customization]] — derivation must satisfy these interaction constraints or produce incoherent systems; justification chains are the mechanism for verifying coherence
- [[four abstraction layers separate platform-agnostic from platform-dependent knowledge system features]] — layer dependencies are one mechanism of dimension interaction: automation-level choices cascade through processing intensity and schema density
- [[enforcing atomicity can create paralysis when ideas resist decomposition]] — documents the creation-time cost at the atomic pole, which this note predicts: fine granularity creates pressure toward heavy processing, and when that processing burden exceeds capacity the system collapses
- [[decontextualization risk means atomicity may strip meaning that cannot be recovered]] — documents the retrieval-time cost at the atomic pole: fine granularity strips argumentative context during extraction, and the coupling means the heavy processing that atomicity demands may not compensate for what extraction loses
- [[small-world topology requires hubs and dense local links]] — the topological consequence of granularity choice: atomic notes require dense local linking to maintain navigability, which is exactly the pressure this note describes
- [[navigational vertigo emerges in pure association systems without local hierarchy]] — the failure mode when granularity-navigation coupling is violated: atomic notes without the deep navigation they demand become unnavigable
- [[complex systems evolve from simple working systems]] — complementary constraint: even at a coherent configuration point, Gall's Law says start simple and evolve; interaction pressures are evolutionary pressures that drive correction or collapse
- [[skill context budgets constrain knowledge system complexity on agent platforms]] — concrete instance of the automation cascade: skill budgets constrain methodology encoding which cascades through schema density and processing intensity
- [[methodology development should follow the trajectory from documentation to skill to hook as understanding hardens]] — the trajectory IS an automation-level progression; this note explains why different automation levels create different pressures on schema density and processing intensity
- [[storage versus thinking distinction determines which tool patterns apply]] — upstream classification that determines which coherence region is viable; storage and thinking systems occupy different interaction-resolved regions of the configuration space
- [[every knowledge domain shares a four-phase processing skeleton that diverges only in the process step]] — explains why the processing intensity cascade is particularly consequential: since only one of four pipeline phases varies by domain, dimension interactions concentrate at that single variable point
- [[novel domains derive by mapping knowledge type to closest reference domain then adapting]] — concrete application of dimension coupling during derivation: adapting a reference domain for a novel use case must respect interaction constraints because changing one dimension (e.g., temporal dynamics) cascades through maintenance cadence, navigation depth, and schema density
- [[multi-domain systems compose through separate templates and shared graph]] — cross-domain pressure: adding a second domain does not simply double the configuration space because dimension interactions propagate across domain boundaries, and the shared graph must accommodate domains with different structural densities
- [[derived systems follow a seed-evolve-reseed lifecycle]] — temporal consequence of coupling: accumulated incremental adaptations in coupled dimensions drift configuration into incoherent regions, explaining why evolution alone is insufficient and principled restructuring (reseeding) is periodically required
- [[configuration paralysis emerges when derivation surfaces too many decisions]] — the UX consequence of coupling: dimension interactions are precisely what makes inference tractable, because resolving a few primary choices propagates through the coupling structure to determine secondary ones, reducing the decision surface from the full combinatorial product to genuine choice points
- [[premature complexity is the most common derivation failure mode]] — dimension coupling is the amplification mechanism: each choice beyond minimum viable cascades through neighbors, so small increases in initial complexity produce disproportionate system-level complexity, making the complexity budget more critical than a linear count would suggest

Topics:
- [[design-dimensions]]
