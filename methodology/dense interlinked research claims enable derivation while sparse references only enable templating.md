---
description: Four structural properties of TFT research — atomic composability, dense interlinking, methodology provenance, and semantic queryability — are prerequisites that separate principled derivation from
kind: research
topics: ["[[design-dimensions]]"]
methodology: ["Original"]
source: [[arscontexta-notes]]
---

# dense interlinked research claims enable derivation while sparse references only enable templating

Derivation — the ability to generate a custom knowledge system from principles rather than copying a template — is not a process you can run against any collection of notes about knowledge management. It requires the research substrate itself to have specific structural properties. Without these properties, the agent falls back to the only thing sparse research supports: selecting and customizing pre-built templates. The difference between derivation and templating is not a philosophical choice about how to distribute systems. It is a structural consequence of how the underlying research is organized.

Four properties separate a derivation-ready research graph from a reference library that can only support templating. These properties transfer across domains because since [[the vault methodology transfers because it encodes cognitive science not domain specifics]], each structural property maps to a cognitive operation rather than a domain-specific convention. And they operate on a fixed architectural inventory: since [[knowledge systems share universal operations and structural components across all methodology traditions]], the nodes that need to be densely interlinked are the eight universal operations and nine structural components that every viable system implements.

**Atomic composability** means each claim stands alone as a linkable unit. Because [[concept-orientation beats source-orientation for cross-domain connections]], the extraction step that produces these independent claims is what makes composability possible — source-bundled research resists composition because its internal structure assumes a specific reading order. When a derivation agent needs to reason about note granularity for a creative writing use case, it should be able to pull [[enforcing atomicity can create paralysis when ideas resist decomposition]] independently from [[decontextualization risk means atomicity may strip meaning that cannot be recovered]] and compose them into a trade-off analysis specific to that domain. If these insights were buried inside a monolithic "guide to atomic notes," the agent would have to load the entire guide and extract the relevant portions — a process that degrades under context window constraints and loses the composability that makes derivation tractable. Atomic claims compose because they can be assembled in novel combinations the original author never imagined. Monolithic references resist composition because their internal structure assumes a specific reading order.

**Dense interlinking** means the claims explain their relationships to each other. Since [[configuration dimensions interact so choices in one create pressure on others]], derivation must understand how choosing atomic granularity creates pressure toward explicit linking and heavy processing. These interaction effects are not derivable from individual claims in isolation — they require link context that articulates why one claim constrains or enables another. A research graph where claims exist as isolated statements forces the derivation agent to infer interactions, which is unreliable because the interactions often reflect empirical discoveries rather than logical necessities. Dense links with context phrases ("extends," "constrains," "contradicts") encode the interaction knowledge that derivation depends on but cannot reconstruct from scratch. This is why [[propositional link semantics transform wiki links from associative to reasoned]] — the move from "these are related" to "this constrains that" is what makes the link graph a reasoning substrate rather than an address book.

**Methodology provenance** means each claim traces to the tradition or discipline that produced it. When [[methodology traditions are named points in a shared configuration space not competing paradigms]], the derivation agent needs to know which tradition validated a claim — because a claim validated by Zettelkasten practice may not transfer to a GTD-style system. Provenance tagging turns the research graph from a flat collection of assertions into a map of which traditions have explored which regions of the configuration space. Since [[source attribution enables tracing claims to foundations]], the vault implements this through YAML provenance fields (`methodology`, `adapted_from`) and Source footers that create a verification graph parallel to the conceptual graph. Without provenance, the derivation agent treats all claims as equally applicable everywhere, which is exactly the false universalism that produces systems technically correct but contextually inappropriate.

**Semantic queryability** means the agent can find relevant claims through meaning, not just keywords. A user building a therapy practice management system would not know to search for "atomic granularity" or "processing cadence." They would describe their needs in domain language — "tracking client progress across sessions" — and the research graph must surface the relevant claims about temporal dynamics, maintenance cadence, and privacy-constrained linking. Because [[spreading activation models how agents should traverse]], semantic search is the entry point that starts the traversal: finding one relevant claim and then following its links to discover the cluster of related claims that together inform the derivation. Keyword search alone cannot bridge the vocabulary gap between domain practitioners and knowledge system researchers.

The structural consequence is that templates and derivation sit on opposite sides of a research quality threshold. Below the threshold — where research is sparse, disconnected, unprovenance-tagged, and keyword-searchable only — the best an agent can do is pattern-match the user's needs to the closest available template and customize. Above the threshold, the agent can traverse the claim graph, compose dimension-specific trade-off analyses, and produce a novel configuration justified at every step — and since [[justification chains enable forward backward and evolution reasoning about configuration decisions]], every configuration choice traces back through the dense claim graph to the specific research claims and user constraints that produced it. The threshold is not binary but the transition is sharp: partial density produces derivations that look principled but have gaps where the agent reverts to guessing because claims run out.

This creates a strategic imperative for the research graph: growth should prioritize density and interlinking over breadth. Adding a new claim that connects to five existing claims strengthens derivation more than adding five disconnected claims about separate topics. Since [[each new note compounds value by creating traversal paths]] and [[small-world topology requires hubs and dense local links]], the graph's derivation capacity grows with connection density, not note count — and that density must exhibit hub structure where synthesis notes and MOCs create the shortcuts that make the claim graph navigable at scale. A hundred deeply interlinked claims about processing patterns support richer derivation than a thousand isolated observations covering the full design space but with no connection structure.

The shadow side is that these structural requirements create a chicken-and-egg problem. You cannot derive systems until the research graph is dense enough, but the graph gets dense through operational experience with derived systems. The resolution is that derivation capacity emerges gradually: early derivations are partial (some dimensions are well-supported, others fall back to templates), and each deployment cycle enriches the graph through operational observations. The research graph is not a prerequisite to be completed before derivation begins — it is a substrate that makes derivation progressively more capable as it grows.

---
---

Relevant Notes:
- [[derivation generates knowledge systems from composable research claims not template customization]] — describes the derivation process itself; this note identifies what the research substrate must look like for that process to function
- [[eight configuration dimensions parameterize the space of possible knowledge systems]] — the dimensions derivation navigates, but navigating them requires claims dense enough to inform each dimension's trade-offs
- [[the derivation engine improves recursively as deployed systems generate observations]] — recursive improvement enriches the research graph with deployment evidence, directly strengthening the four properties this note identifies as prerequisites
- [[bootstrapping principle enables self-improving systems]] — the self-referential loop where the research graph improves itself by processing its own operational evidence, which is exactly how claim density and interlinking grow over time
- [[composable knowledge architecture builds systems from independent toggleable modules not monolithic templates]] — composable architecture is the engineering output; this note argues that composable research INPUT is equally necessary
- [[source attribution enables tracing claims to foundations]] — implements the methodology provenance property: provenance tagging requires attribution infrastructure that traces each claim to its tradition and source
- [[propositional link semantics transform wiki links from associative to reasoned]] — operationalizes the dense interlinking property: context phrases like 'extends' and 'constrains' are propositional semantics that encode the interaction knowledge derivation depends on
- [[concept-orientation beats source-orientation for cross-domain connections]] — enables the atomic composability property: extracting concept-oriented claims from source material is the act that produces independently composable units
- [[justification chains enable forward backward and evolution reasoning about configuration decisions]] — downstream consumer of all four properties: chain quality is a trailing indicator of claim graph quality because chains can only trace what the graph's density and provenance make traceable
- [[small-world topology requires hubs and dense local links]] — the graph topology that makes dense interlinking navigable: without power-law distribution creating shortcuts through hubs, dense interlinks would exist but remain opaque to traversal
- [[knowledge systems share universal operations and structural components across all methodology traditions]] — the universal operations and components are the nodes that dense interlinking connects; derivation-readiness requires density across this fixed inventory rather than breadth into novel categories
- [[the vault methodology transfers because it encodes cognitive science not domain specifics]] — grounds WHY the four substrate properties work across domains: atomic composability, dense interlinking, provenance, and queryability transfer because they encode cognitive operations not domain content

Topics:
- [[design-dimensions]]
